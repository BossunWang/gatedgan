{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import hiddenlayer as hl\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'window_3753268d5a6c5a'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import visdom\n",
    "import numpy as np\n",
    "vis = visdom.Visdom()\n",
    "vis.text('Hello, world!')\n",
    "vis.image(np.ones((3, 10, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from visdom import Visdom\n",
    "import numpy as np\n",
    "\n",
    "def tensor2image(tensor):\n",
    "    image = 127.5*(tensor[0].cpu().float().numpy() + 1.0)\n",
    "    if image.shape[0] == 1:\n",
    "        image = np.tile(image, (3,1,1))\n",
    "    return image.astype(np.uint8)\n",
    "\n",
    "class Logger():\n",
    "    def __init__(self, n_epochs, batches_epoch):\n",
    "        self.viz = Visdom()\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batches_epoch = batches_epoch\n",
    "        self.epoch = 1\n",
    "        self.batch = 1\n",
    "        self.prev_time = time.time()\n",
    "        self.mean_period = 0\n",
    "        self.losses = {}\n",
    "        self.loss_windows = {}\n",
    "        self.image_windows = {}\n",
    "\n",
    "\n",
    "    def log(self, losses=None, images=None):\n",
    "        self.mean_period += (time.time() - self.prev_time)\n",
    "        self.prev_time = time.time()\n",
    "\n",
    "        sys.stdout.write('\\rEpoch %03d/%03d [%04d/%04d] -- ' % (self.epoch, self.n_epochs, self.batch, self.batches_epoch))\n",
    "\n",
    "        for i, loss_name in enumerate(losses.keys()):\n",
    "            if loss_name not in self.losses:\n",
    "                self.losses[loss_name] = losses[loss_name].data[0]\n",
    "            else:\n",
    "                self.losses[loss_name] += losses[loss_name].data[0]\n",
    "\n",
    "            if (i+1) == len(losses.keys()):\n",
    "                sys.stdout.write('%s: %.4f -- ' % (loss_name, self.losses[loss_name]/self.batch))\n",
    "            else:\n",
    "                sys.stdout.write('%s: %.4f | ' % (loss_name, self.losses[loss_name]/self.batch))\n",
    "\n",
    "        batches_done = self.batches_epoch*(self.epoch - 1) + self.batch\n",
    "        batches_left = self.batches_epoch*(self.n_epochs - self.epoch) + self.batches_epoch - self.batch \n",
    "        sys.stdout.write('ETA: %s' % (datetime.timedelta(seconds=batches_left*self.mean_period/batches_done)))\n",
    "\n",
    "        # Draw images\n",
    "        for image_name, tensor in images.items():\n",
    "            if image_name not in self.image_windows:\n",
    "                self.image_windows[image_name] = self.viz.image(tensor2image(tensor.data), opts={'title':image_name})\n",
    "            else:\n",
    "                self.viz.image(tensor2image(tensor.data), win=self.image_windows[image_name], opts={'title':image_name})\n",
    "\n",
    "        # End of epoch\n",
    "        if (self.batch % self.batches_epoch) == 0:\n",
    "            # Plot losses\n",
    "            for loss_name, loss in self.losses.items():\n",
    "                if loss_name not in self.loss_windows:\n",
    "                    self.loss_windows[loss_name] = self.viz.line(X=np.array([self.epoch]), Y=np.array([loss/self.batch]), \n",
    "                                                                    opts={'xlabel': 'epochs', 'ylabel': loss_name, 'title': loss_name})\n",
    "                else:\n",
    "                    self.viz.line(X=np.array([self.epoch]), Y=np.array([loss/self.batch]), win=self.loss_windows[loss_name], update='append')\n",
    "                # Reset losses for next epoch\n",
    "                self.losses[loss_name] = 0.0\n",
    "\n",
    "            self.epoch += 1\n",
    "            self.batch = 1\n",
    "            sys.stdout.write('\\n')\n",
    "        else:\n",
    "            self.batch += 1\n",
    "            \n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, mode='train'):\n",
    "        transforms_ = [ transforms.Resize(int(143), Image.BICUBIC), \n",
    "                transforms.RandomCrop(128), \n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) \n",
    "              ]\n",
    "        \n",
    "        self.transform = transforms.Compose(transforms_)        \n",
    "        #content image source\n",
    "        self.X = sorted(glob.glob(os.path.join(root, f'{mode}Content', '*')))\n",
    "        \n",
    "        self.Y = []\n",
    "        #style image source(s)\n",
    "        style_sources = sorted(glob.glob(os.path.join(root, f'{mode}Styles', '*')))\n",
    "        for label,style in enumerate(style_sources):\n",
    "            temp = [(label,x) for x in sorted(glob.glob(style_sources[label]+\"/*\"))]\n",
    "            self.Y.extend(temp)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        output = {}\n",
    "        output['content'] = self.transform(Image.open(self.X[index % len(self.X)]))\n",
    "        \n",
    "        #select style\n",
    "        selection = self.Y[random.randint(0, len(self.Y) - 1)]\n",
    "        output['style'] = self.transform(Image.open(selection[1]))\n",
    "        output['style_label'] = selection[0]\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(len(self.X), len(self.Y))\n",
    "        \n",
    "        \n",
    "        \n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0,1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size-1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))\n",
    "\n",
    "class LambdaLR():\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant(m.bias.data, 0.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/jxgu1016/Total_Variation_Loss.pytorch/blob/master/TVLoss.py\n",
    "class TVLoss(nn.Module):\n",
    "    def __init__(self,TVLoss_weight=tv_strength or None):\n",
    "        super(TVLoss,self).__init__()\n",
    "        self.TVLoss_weight = TVLoss_weight\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h = self._tensor_size(x[:,:,1:,:])\n",
    "        count_w = self._tensor_size(x[:,:,:,1:])\n",
    "        h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum()\n",
    "        w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum()\n",
    "        return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size\n",
    "\n",
    "    def _tensor_size(self,t):\n",
    "        return t.size()[1]*t.size()[2]*t.size()[3]\n",
    "    \n",
    "def label2tensor(label,tensor):\n",
    "    for i in range(label.size(0)):\n",
    "        tensor[i].fill_(label[i])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_features):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        conv_block = [  nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features)  ]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x) #skip connection    \n",
    "\n",
    "class Encoder(nn.Module):    \n",
    "    def __init__(self, in_nc, ngf=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        #Inital Conv Block\n",
    "        model = [   nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(in_nc, ngf, 7),\n",
    "                    nn.InstanceNorm2d(ngf),\n",
    "                    nn.ReLU(inplace=True) ]\n",
    "        \n",
    "        in_features = ngf\n",
    "        out_features = in_features *2\n",
    "        \n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)    \n",
    "            ]\n",
    "            \n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "            \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Return batch w/ encoded content picture\n",
    "        return [self.model(x['content']),\n",
    "               x['style_label']]\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,n_styles, ngf,auto_id=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        #nclasses = input_nclasses\n",
    "        self.t = nn.ModuleList([ResidualBlock(ngf*4) for i in range(n_styles)])\n",
    "        if auto_id:\n",
    "            self.t.append(Identity())\n",
    "        #self.i = Identity()\n",
    "                \n",
    "    def forward(self,x):\n",
    "        #x0 is content, x1 is label \n",
    "        label = x[1][0]\n",
    "#         print(label)\n",
    "#         print(len(label))\n",
    "#         print(label.shape)\n",
    "        mix = sum([self.t[i](x[0])*v for (i,v) in enumerate(label)])\n",
    "        #return content transformed by style specific residual block \n",
    "        return mix\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_nc, ngf, n_residual_blocks=9):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        in_features = ngf * 4\n",
    "        out_features = in_features//2\n",
    "        \n",
    "        model = []\n",
    "        #ResBlockLand\n",
    "        \n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            model += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                        nn.InstanceNorm2d(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "\n",
    "        # Output layer\n",
    "        model += [  nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(64, out_nc, 7),\n",
    "                    nn.Tanh() ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,in_nc,out_nc,n_styles,ngf):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(in_nc,ngf)\n",
    "        self.transformer = Transformer(n_styles,ngf)\n",
    "        self.decoder = Decoder(out_nc,ngf)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Pass generator batch of {content=,style=?,style_label=}\n",
    "        print('generator shape/style in: ',x['content'].shape, x['style_label'])\n",
    "        \n",
    "        e = self.encoder(x)\n",
    "        print(e[0].shape,e[1])\n",
    "        t = self.transformer(e)\n",
    "        print(t.shape)\n",
    "        d = self.decoder(t)\n",
    "        print(d.shape)\n",
    "        return d\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc, n_styles, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # A bunch of convolutions one after another\n",
    "        model = [   nn.Conv2d(in_nc, 64, 4, stride=2, padding=1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(128), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(256), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(256, 512, 4, padding=1),\n",
    "                    nn.InstanceNorm2d(512), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "        \n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "        # FCN classification layer-\n",
    "        self.fldiscriminator = nn.Conv2d(512, 1, 4, padding=1)\n",
    "        \n",
    "        # aux class layer\n",
    "        self.aux_clf = nn.Conv2d(512, n_styles, 4, padding = 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base =  self.model(x)\n",
    "        print('base shape: ',base.shape)\n",
    "        discrim = self.fldiscriminator(base)\n",
    "        print('init discrim shape: ',discrim.shape)\n",
    "        # Average pooling and flatten\n",
    "        discrim=F.avg_pool2d(discrim, discrim.size()[2:]).view(discrim.size()[0], -1) \n",
    "        print('preview discrim: ',discrim)\n",
    "        print('preview discrim shape: ',discrim.size())\n",
    "        discrim = discrim.view(-1)\n",
    "        print('discrim shape: ',discrim.size())\n",
    "        print('discrim: ',discrim)\n",
    "        clf = self.aux_clf(base).transpose_(1,3)\n",
    "        print('clf shape: ',clf.shape)\n",
    "        print('clf transpose: ',clf.transpose_(1,3).shape)\n",
    "        \n",
    "        return [discrim,clf.transpose_(1,3)]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN OPTIONS FROM GATED GAN\n",
    "epoch = 0\n",
    "n_epochs = 10 #default = 200\n",
    "batchSize = 1\n",
    "dataroot = './photo2fourcollection'\n",
    "batchSize = 1\n",
    "loadSize = 143\n",
    "fineSize = 128\n",
    "ngf = 64\n",
    "ndf = 64    \n",
    "in_nc = 3 \n",
    "out_nc = 3 \n",
    "niter = 100  \n",
    "niter_decay = 100 \n",
    "lr = 0.0002 \n",
    "beta1 = 0.5 \n",
    "#ntrain = math.huge \n",
    "flip = 1  \n",
    "display_id = 10 \n",
    "display_winsize = 128 \n",
    "display_freq = 25 \n",
    "gpu = 1 \n",
    "name = ''   \n",
    "which_direction = 'AtoB'\n",
    "phase = 'train'\n",
    "nThreads = 2\n",
    "save_epoch_freq = 1\n",
    "save_latest_freq = 5000 \n",
    "print_freq = 50\n",
    "save_display_feq = 2500\n",
    "continue_train = 0\n",
    "serial_batches = 0\n",
    "checkpoints_dir = './checkpoints'\n",
    "cudnn = 1\n",
    "which_model_netD = 'basic'\n",
    "which_model_netG = 'auto_gated_resnet_6blocks'\n",
    "norm = 'instance'\n",
    "n_layers_D = 3\n",
    "lambda_A = 10.0\n",
    "lambda_B = 10.0\n",
    "model = 'gated_gan'\n",
    "use_lsgan = 1\n",
    "align_data = 0\n",
    "pool_size = 50\n",
    "resize_or_crop = 'resize_and_crop'\n",
    "autoencoder_constrain = 10 \n",
    "n_styles = 4\n",
    "test_data_path = ''\n",
    "decay_epoch=1\n",
    "cuda=False\n",
    "tv_strength=1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ImageDataset('./photo2fourcollection'), \n",
    "                        batch_size=1, shuffle=True)\n",
    "batch = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(in_nc, out_nc, n_styles, ngf)\n",
    "discriminator= Discriminator(in_nc,n_styles, ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Losses Init\n",
    "use_lsgan=True\n",
    "if use_lsgan:\n",
    "    criterion_GAN = nn.MSELoss()\n",
    "else: \n",
    "    criterion_GAN = nn.BCELoss()\n",
    "    \n",
    "    \n",
    "criterion_ACGAN = nn.CrossEntropyLoss(weight=None)\n",
    "criterion_Rec = nn.L1Loss()\n",
    "criterion_Enc = nn.MSELoss()\n",
    "criterion_TV = TVLoss(TVLoss_weight=tv_strength)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizers & LR schedulers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(),\n",
    "                                lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), \n",
    "                               lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch,decay_epoch).step)\n",
    "lr_scheduler_D = torch.optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=LambdaLR(n_epochs,epoch, decay_epoch).step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base shape:  torch.Size([1, 512, 15, 15])\n",
      "init discrim shape:  torch.Size([1, 1, 14, 14])\n",
      "preview discrim:  tensor([[0.3353]], grad_fn=<ViewBackward>)\n",
      "preview discrim shape:  torch.Size([1, 1])\n",
      "discrim shape:  torch.Size([1])\n",
      "discrim:  tensor([0.3353], grad_fn=<ViewBackward>)\n",
      "clf shape:  torch.Size([1, 20, 20, 4])\n",
      "clf transpose:  torch.Size([1, 4, 20, 20])\n",
      "base shape:  torch.Size([1, 512, 15, 15])\n",
      "init discrim shape:  torch.Size([1, 1, 14, 14])\n",
      "preview discrim:  tensor([[0.3353]], grad_fn=<ViewBackward>)\n",
      "preview discrim shape:  torch.Size([1, 1])\n",
      "discrim shape:  torch.Size([1])\n",
      "discrim:  tensor([0.3353], grad_fn=<ViewBackward>)\n",
      "clf shape:  torch.Size([1, 20, 20, 4])\n",
      "clf transpose:  torch.Size([1, 4, 20, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  )\n",
       "  (fldiscriminator): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  (aux_clf): Conv2d(512, 4, kernel_size=(4, 4), stride=(1, 1), padding=(4, 4))\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set vars for training\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "input_A = Tensor(batchSize, in_nc, fineSize, fineSize)\n",
    "input_B = Tensor(batchSize, out_nc, fineSize, fineSize)\n",
    "target_real = Variable(Tensor(batchSize).fill_(1.0), requires_grad=False)\n",
    "target_fake = Variable(Tensor(batchSize).fill_(0.0), requires_grad=False)\n",
    "\n",
    "D_A_size = discriminator(real_style)[0].size()  \n",
    "D_AC_size = discriminator(real_style)[1].size()\n",
    "\n",
    "class_label_B = torch.Tensor(D_AC_size[0],D_AC_size[1],D_AC_size[2]).long()\n",
    "\n",
    "autoflag_OHE = torch.Tensor(1,n_styles+1).fill_(0).long()\n",
    "autoflag_OHE[0][-1] = 1\n",
    "\n",
    "fake_label = torch.Tensor(D_A_size).fill_(0.0)\n",
    "real_label = torch.Tensor(D_A_size).fill_(0.9) \n",
    "\n",
    "rec_A_AE = torch.Tensor(batchSize,in_nc,fineSize,fineSize)\n",
    "\n",
    "fake_buffer = ReplayBuffer()\n",
    "\n",
    "##INIT WEIGHTS\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_content = Variable(input_A.copy_(batch['content']))\n",
    "real_style = Variable(input_B.copy_(batch['style']))\n",
    "style_label = batch['style_label']\n",
    "style_OHE = F.one_hot(style_label,n_styles).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(n_epochs, len(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator shape/style in:  torch.Size([1, 3, 128, 128]) tensor([[0, 0, 0, 0, 1]])\n",
      "torch.Size([1, 256, 32, 32]) tensor([[0, 0, 0, 0, 1]])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "generator shape/style in:  torch.Size([1, 3, 128, 128]) tensor([[0, 1, 0, 0]])\n",
      "torch.Size([1, 256, 32, 32]) tensor([[0, 1, 0, 0]])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 3, 128, 128])\n",
      "base shape:  torch.Size([1, 512, 15, 15])\n",
      "init discrim shape:  torch.Size([1, 1, 14, 14])\n",
      "preview discrim:  tensor([[0.5494]], grad_fn=<ViewBackward>)\n",
      "preview discrim shape:  torch.Size([1, 1])\n",
      "discrim shape:  torch.Size([1])\n",
      "discrim:  tensor([0.5494], grad_fn=<ViewBackward>)\n",
      "clf shape:  torch.Size([1, 20, 20, 4])\n",
      "clf transpose:  torch.Size([1, 4, 20, 20])\n",
      "base shape:  torch.Size([1, 512, 15, 15])\n",
      "init discrim shape:  torch.Size([1, 1, 14, 14])\n",
      "preview discrim:  tensor([[1.2272]], grad_fn=<ViewBackward>)\n",
      "preview discrim shape:  torch.Size([1, 1])\n",
      "discrim shape:  torch.Size([1])\n",
      "discrim:  tensor([1.2272], grad_fn=<ViewBackward>)\n",
      "clf shape:  torch.Size([1, 20, 20, 4])\n",
      "clf transpose:  torch.Size([1, 4, 20, 20])\n",
      "base shape:  torch.Size([1, 512, 15, 15])\n",
      "init discrim shape:  torch.Size([1, 1, 14, 14])\n",
      "preview discrim:  tensor([[0.5494]], grad_fn=<ViewBackward>)\n",
      "preview discrim shape:  torch.Size([1, 1])\n",
      "discrim shape:  torch.Size([1])\n",
      "discrim:  tensor([0.5494], grad_fn=<ViewBackward>)\n",
      "clf shape:  torch.Size([1, 20, 20, 4])\n",
      "clf transpose:  torch.Size([1, 4, 20, 20])\n",
      "Epoch 001/010 [0001/6287] -- "
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-464116201d06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0;34m'loss_G_AC'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0merrG_AC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss_D'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0merrD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'errD_fake'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0merrD_fake\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                    'errD_real':errD_real, 'errD_class': errD_real_class}, \n\u001b[0;32m---> 59\u001b[0;31m                     images={'content': real_content, 'style': real_style, 'transfer': genfake})\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m##update learning rates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c593d05b746b>\u001b[0m in \u001b[0;36mlog\u001b[0;34m(self, losses, images)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
     ]
    }
   ],
   "source": [
    "###FUCKIN TRAIN TIME BABY\n",
    "for epoch in range(epoch,n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        real_content = Variable(input_A.copy_(batch['content']))\n",
    "        real_style = Variable(input_B.copy_(batch['style']))\n",
    "        style_label = batch['style_label']\n",
    "        style_OHE = F.one_hot(style_label,n_styles).long()\n",
    "        \n",
    "        #### GENERATOR FORWARD\n",
    "        optimizer_G.zero_grad()\n",
    "        #Auto Encoder Reconstruction Loss\n",
    "        rec = generator({\n",
    "            'content':real_content,\n",
    "            'style_label': autoflag_OHE # 1-n_styles +1 ((nstyles))\n",
    "        })\n",
    "        errRec = criterion_Rec(rec,real_content)\n",
    "\n",
    "        #Gan Loss\n",
    "        genfake = generator({\n",
    "            'content':real_content,\n",
    "            'style_label': style_OHE\n",
    "        })\n",
    "        output = discriminator(genfake)\n",
    "        errG = criterion_GAN(output[0],torch.tensor([0.]))\n",
    "\n",
    "        #https://github.com/pytorch/pytorch/issues/29\n",
    "        #Aux Class Loss\n",
    "        errG_AC = criterion_ACGAN(output[1].transpose_(1,3),class_label_B)\n",
    "\n",
    "        tvloss = criterion_TV(genfake)\n",
    "        errG_total = errRec*autoencoder_constrain + errG_AC*lambda_A + errG + tvloss\n",
    "\n",
    "        errG_total.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        #### DISCRIMINATOR FORWARD\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        #Real Loss\n",
    "        output = discriminator(real_style)\n",
    "        errD_real = criterion_GAN(output[0],torch.tensor([.9]))\n",
    "\n",
    "        errD_real_class = criterion_ACGAN(output[1].transpose(1,3),class_label_B)\n",
    "\n",
    "        #Fake Loss\n",
    "        fake = fake_buffer.push_and_pop(genfake)\n",
    "        out_real, out_class = discriminator(fake)\n",
    "        errD_fake = criterion_GAN(out_real,torch.tensor([0.]))\n",
    "\n",
    "        errD = ((errD_real+errD_fake)/2.0)+errD_real_class\n",
    "        errD.backward()\n",
    "\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        #Progress report (port 8097)\n",
    "        logger.log({'loss_G': errG_total, 'loss_G_AE': errRec, 'loss_G_GAN': errG,\n",
    "                    'loss_G_AC': errG_AC, 'loss_D': errD, 'errD_fake':errD_fake,\n",
    "                   'errD_real':errD_real, 'errD_class': errD_real_class}, \n",
    "                    images={'content': real_content, 'style': real_style, 'transfer': genfake})\n",
    "    \n",
    "    ##update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    \n",
    "    #Save model\n",
    "    torch.save(netG.state_dict(), 'output/netG.pth')\n",
    "    torch.save(netD.state_dict(), 'output/netD.pth')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
